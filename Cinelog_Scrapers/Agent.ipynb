{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "748c5429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Processing: Ala Vaikunthapurramuloo\n",
      "ðŸŽ¬ Processing: Arjun Reddy\n",
      "ðŸŽ¬ Processing: F2: Fun and Frustration\n",
      "ðŸŽ¬ Processing: Maharshi\n",
      "ðŸŽ¬ Processing: Bheeshma\n",
      "ðŸŽ¬ Processing: Saaho\n",
      "ðŸŽ¬ Processing: Kabir Singh\n",
      "ðŸŽ¬ Processing: Geetha Govindam\n",
      "ðŸŽ¬ Processing: Mahanati\n",
      "ðŸŽ¬ Processing: Rangasthalam\n",
      "ðŸŽ¬ Processing: Sye Raa Narasimha Reddy\n",
      "ðŸŽ¬ Processing: Nenu Sailaja\n",
      "ðŸŽ¬ Processing: Pelli Choopulu\n",
      "ðŸŽ¬ Processing: Dookudu\n",
      "ðŸŽ¬ Processing: Mirchi\n",
      "ðŸŽ¬ Processing: Businessman\n",
      "ðŸŽ¬ Processing: Autonagar Surya\n",
      "ðŸŽ¬ Processing: Premam\n",
      "ðŸŽ¬ Processing: Gentleman\n",
      "ðŸŽ¬ Processing: Kalyana Vaibhogame\n",
      "ðŸŽ¬ Processing: Srimanthudu\n",
      "ðŸŽ¬ Processing: Attarintiki Daredi\n",
      "ðŸŽ¬ Processing: Baahubali 2: The Conclusion\n",
      "ðŸŽ¬ Processing: Baahubali\n",
      "ðŸŽ¬ Processing: Ye Maaya Chesave\n",
      "ðŸŽ¬ Processing: Eega\n",
      "ðŸŽ¬ Processing: Dilwala\n",
      "ðŸŽ¬ Processing: Brahma\n",
      "ðŸŽ¬ Processing: Maheshinte Prathikaaram\n",
      "ðŸŽ¬ Processing: Aagadu\n",
      "ðŸŽ¬ Processing: Jai Lava Kusa\n",
      "ðŸŽ¬ Processing: Taxiwala\n",
      "ðŸŽ¬ Processing: RX100\n",
      "ðŸŽ¬ Processing: Kabali\n",
      "ðŸŽ¬ Processing: Janatha Garage\n",
      "ðŸŽ¬ Processing: Gaddalakonda Ganesh\n",
      "ðŸŽ¬ Processing: A..Aa\n",
      "ðŸŽ¬ Processing: Jawaan\n",
      "ðŸŽ¬ Processing: Naa Peru Surya\n",
      "ðŸŽ¬ Processing: Kalyana Vaibhogam\n",
      "ðŸŽ¬ Processing: LIE\n",
      "ðŸŽ¬ Processing: Sarrainodu\n",
      "ðŸŽ¬ Processing: Tippu\n",
      "ðŸŽ¬ Processing: D for Dopidi\n",
      "ðŸŽ¬ Processing: Gopala Gopala\n",
      "ðŸŽ¬ Processing: Courier Boy Kalyan\n",
      "ðŸŽ¬ Processing: Vedalam\n",
      "ðŸŽ¬ Processing: Anjali\n",
      "ðŸŽ¬ Processing: Pelli Sandadi\n",
      "ðŸŽ¬ Processing: Khaidi No.150\n",
      "ðŸŽ¬ Processing: Okkadu\n",
      "ðŸŽ¬ Processing: Gharshana\n",
      "ðŸŽ¬ Processing: Aadukalam\n",
      "ðŸŽ¬ Processing: Jabardasth\n",
      "ðŸŽ¬ Processing: Oxygen\n",
      "ðŸŽ¬ Processing: Thikka\n",
      "ðŸŽ¬ Processing: Kantri\n",
      "ðŸŽ¬ Processing: Aaha Kalyanam\n",
      "ðŸŽ¬ Processing: Khiladi Krishna\n",
      "ðŸŽ¬ Processing: Yatra\n",
      "ðŸŽ¬ Processing: Vunnadhi Okate Zindagi\n",
      "ðŸŽ¬ Processing: Raju Gadu\n",
      "ðŸŽ¬ Processing: Sneha Geetham\n",
      "ðŸŽ¬ Processing: Gaganam\n",
      "ðŸŽ¬ Processing: Arundhati\n",
      "ðŸŽ¬ Processing: Lakshmi Bomb\n",
      "ðŸŽ¬ Processing: Gajendra\n",
      "âœ… Data written to movies.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# === Load API keys ===\n",
    "load_dotenv()\n",
    "OMDB_KEY = '2774b611'  # Replace if needed\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# === Functions ===\n",
    "def fetch_omdb_data(title: str) -> dict:\n",
    "    url = f\"http://www.omdbapi.com/?t={title}&apikey={OMDB_KEY}\"\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "        if data.get(\"Response\") == \"True\":\n",
    "            return data\n",
    "    return {\"Error\": f\"No data found for {title}\"}\n",
    "\n",
    "def generate_tags(plot_and_genre: str) -> str:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Generate 3â€“5 lowercase tags (no hashtags or punctuation), separated by '|'.\n",
    "Do not explain or include extra text.\n",
    "\n",
    "Plot and Genre:\n",
    "{plot_and_genre}\n",
    "\n",
    "Tags:\n",
    "\"\"\")\n",
    "    _input = prompt.format(plot_and_genre=plot_and_genre)\n",
    "    result = llm.predict(_input).strip()\n",
    "    lines = result.splitlines()\n",
    "    tags = lines[-1].strip()\n",
    "    if \":\" in tags:\n",
    "        tags = tags.split(\":\")[-1].strip()\n",
    "    return tags.replace('\"', '').strip('.')\n",
    "\n",
    "# === Read movie titles ===\n",
    "with open(\"movies.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    movie_titles = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# === Output file (JSONL) ===\n",
    "output_file = \"movies.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for title in movie_titles:\n",
    "        print(f\"ðŸŽ¬ Processing: {title}\")\n",
    "        movie_data = fetch_omdb_data(title)\n",
    "        if \"Error\" in movie_data:\n",
    "            print(f\"âŒ Skipped {title}: {movie_data['Error']}\")\n",
    "            continue\n",
    "\n",
    "        plot = movie_data.get(\"Plot\", \"\")\n",
    "        genre = movie_data.get(\"Genre\", \"\").split(\",\")[0].strip()\n",
    "        tag_input = f\"{plot} Genre: {genre}\"\n",
    "        tags = generate_tags(tag_input)\n",
    "\n",
    "        json_record = {\n",
    "            \"title\": movie_data.get(\"Title\", \"\"),\n",
    "            \"plot\": plot,\n",
    "            \"director\": movie_data.get(\"Director\", \"\"),\n",
    "            \"genre\": genre,\n",
    "            \"rating\": movie_data.get(\"imdbRating\", \"\"),\n",
    "            \"release_date\": movie_data.get(\"Released\", \"\"),\n",
    "            \"language\": \"|\".join(lang.strip() for lang in movie_data.get(\"Language\", \"\").split(\",\")),\n",
    "            \"country\": \"|\".join(c.strip() for c in movie_data.get(\"Country\", \"\").split(\",\")),\n",
    "            \"cast\": \"|\".join(actor.strip() for actor in movie_data.get(\"Actors\", \"\").split(\",\")),\n",
    "            \"tags\": tags\n",
    "        }\n",
    "\n",
    "        outfile.write(json.dumps(json_record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce3567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Generating movie titles for: Top Tollywood (Telugu) movies, recent ones have higher priority\n",
      "ðŸ” Validating titles via OMDb...\n",
      "â© Skipping duplicate: Ala Vaikunthapurramuloo\n",
      "â© Skipping duplicate: Arjun Reddy\n",
      "â© Skipping duplicate: Maharshi\n",
      "â© Skipping duplicate: Rangasthalam\n",
      "â© Skipping duplicate: Nenu Sailaja\n",
      "â© Skipping duplicate: Geetha Govindam\n",
      "â© Skipping duplicate: F2: Fun and Frustration\n",
      "â© Skipping duplicate: Mahanati\n",
      "â© Skipping duplicate: Aagadu\n",
      "â© Skipping duplicate: Srimanthudu\n",
      "â© Skipping duplicate: Gentleman\n",
      "â© Skipping duplicate: Maheshinte Prathikaaram\n",
      "â© Skipping duplicate: Sarrainodu\n",
      "â© Skipping duplicate: Eega\n",
      "â© Skipping duplicate: Aagadu\n",
      "â© Skipping duplicate: Nenu Sailaja\n",
      "â© Skipping duplicate: Mahanati\n",
      "â© Skipping duplicate: Arjun Reddy\n",
      "â© Skipping duplicate: Maharshi\n",
      "â© Skipping duplicate: Srimanthudu\n",
      "â© Skipping duplicate: Khaidi No.150\n",
      "â© Skipping duplicate: Okkadu\n",
      "â© Skipping duplicate: Gharshana\n",
      "â© Skipping duplicate: Aadukalam\n",
      "â© Skipping duplicate: Jabardasth\n",
      "â© Skipping duplicate: Oxygen\n",
      "â© Skipping duplicate: Thikka\n",
      "â© Skipping duplicate: Gentleman\n",
      "â© Skipping duplicate: Kantri\n",
      "â© Skipping duplicate: Aaha Kalyanam\n",
      "â© Skipping duplicate: Khiladi Krishna\n",
      "â© Skipping duplicate: Yatra\n",
      "â© Skipping duplicate: Vunnadhi Okate Zindagi\n",
      "â© Skipping duplicate: Raju Gadu\n",
      "â© Skipping duplicate: Sneha Geetham\n",
      "â© Skipping duplicate: Maheshinte Prathikaaram\n",
      "â© Skipping duplicate: Sarrainodu\n",
      "â© Skipping duplicate: Gaganam\n",
      "â© Skipping duplicate: Arundhati\n",
      "â© Skipping duplicate: Eega\n",
      "â© Skipping duplicate: Aagadu\n",
      "â© Skipping duplicate: Nenu Sailaja\n",
      "â© Skipping duplicate: Lakshmi Bomb\n",
      "â© Skipping duplicate: Mahanati\n",
      "â© Skipping duplicate: Gajendra\n",
      "â© Skipping duplicate: Arjun Reddy\n",
      "â© Skipping duplicate: Maharshi\n",
      "â© Skipping duplicate: Srimanthudu\n",
      "â© Skipping duplicate: Khaidi No.150\n",
      "â© Skipping duplicate: Okkadu\n",
      "â© Skipping duplicate: Gharshana\n",
      "â© Skipping duplicate: Aadukalam\n",
      "â© Skipping duplicate: Jabardasth\n",
      "â© Skipping duplicate: Oxygen\n",
      "â© Skipping duplicate: Thikka\n",
      "â© Skipping duplicate: Gentleman\n",
      "â© Skipping duplicate: Kantri\n",
      "â© Skipping duplicate: Aaha Kalyanam\n",
      "â© Skipping duplicate: Khiladi Krishna\n",
      "â© Skipping duplicate: Yatra\n",
      "â© Skipping duplicate: Vunnadhi Okate Zindagi\n",
      "â© Skipping duplicate: Raju Gadu\n",
      "â© Skipping duplicate: Sneha Geetham\n",
      "â© Skipping duplicate: Maheshinte Prathikaaram\n",
      "â© Skipping duplicate: Sarrainodu\n",
      "â© Skipping duplicate: Gaganam\n",
      "â© Skipping duplicate: Arundhati\n",
      "â© Skipping duplicate: Eega\n",
      "â© Skipping duplicate: Aagadu\n",
      "â© Skipping duplicate: Nenu Sailaja\n",
      "â© Skipping duplicate: Lakshmi Bomb\n",
      "â© Skipping duplicate: Mahanati\n",
      "â© Skipping duplicate: Gajendra\n",
      "â© Skipping duplicate: Arjun Reddy\n",
      "â© Skipping duplicate: Maharshi\n",
      "â© Skipping duplicate: Srimanthudu\n",
      "â© Skipping duplicate: Khaidi No.150\n",
      "â© Skipping duplicate: Okkadu\n",
      "â© Skipping duplicate: Gharshana\n",
      "â© Skipping duplicate: Aadukalam\n",
      "â© Skipping duplicate: Jabardasth\n",
      "â© Skipping duplicate: Oxygen\n",
      "â© Skipping duplicate: Thikka\n",
      "â© Skipping duplicate: Gentleman\n",
      "â© Skipping duplicate: Kantri\n",
      "â© Skipping duplicate: Aaha Kalyanam\n",
      "â© Skipping duplicate: Khiladi Krishna\n",
      "â© Skipping duplicate: Yatra\n",
      "â© Skipping duplicate: Vunnadhi Okate Zindagi\n",
      "â© Skipping duplicate: Raju Gadu\n",
      "â© Skipping duplicate: Sneha Geetham\n",
      "â© Skipping duplicate: Maheshinte Prathikaaram\n",
      "â© Skipping duplicate: Sarrainodu\n",
      "â© Skipping duplicate: Gaganam\n",
      "â© Skipping duplicate: Arundhati\n",
      "â© Skipping duplicate: Eega\n",
      "â© Skipping duplicate: Aagadu\n",
      "â© Skipping duplicate: Nenu Sailaja\n",
      "â© Skipping duplicate: Lakshmi Bomb\n",
      "â© Skipping duplicate: Mahanati\n",
      "â© Skipping duplicate: Gajendra\n",
      "â© Skipping duplicate: Arjun Reddy\n",
      "â© Skipping duplicate: Maharshi\n",
      "â© Skipping duplicate: Srimanthudu\n",
      "â© Skipping duplicate: Khaidi No.150\n",
      "â© Skipping duplicate: Okkadu\n",
      "â© Skipping duplicate: Gharshana\n",
      "â© Skipping duplicate: Aadukalam\n",
      "â© Skipping duplicate: Jabardasth\n",
      "â© Skipping duplicate: Oxygen\n",
      "â© Skipping duplicate: Thikka\n",
      "â© Skipping duplicate: Gentleman\n",
      "â© Skipping duplicate: Kantri\n",
      "â© Skipping duplicate: Aaha Kalyanam\n",
      "â© Skipping duplicate: Khiladi Krishna\n",
      "â© Skipping duplicate: Yatra\n",
      "â© Skipping duplicate: Vunnadhi Okate Zindagi\n",
      "â© Skipping duplicate: Raju Gadu\n",
      "â© Skipping duplicate: Sneha Geetham\n",
      "â© Skipping duplicate: Maheshinte Prathikaaram\n",
      "â© Skipping duplicate: Sarrainodu\n",
      "â© Skipping duplicate: Gaganam\n",
      "â© Skipping duplicate: Arundhati\n",
      "â© Skipping duplicate: Eega\n",
      "â© Skipping duplicate: Aagadu\n",
      "â© Skipping duplicate: Nenu Sailaja\n",
      "â© Skipping duplicate: Lakshmi Bomb\n",
      "â© Skipping duplicate: Mahanati\n",
      "â© Skipping duplicate: Gajendra\n",
      "â© Skipping duplicate: Arjun Reddy\n",
      "â© Skipping duplicate: Maharshi\n",
      "â© Skipping duplicate: Srimanthudu\n",
      "â© Skipping duplicate: Khaidi No.150\n",
      "â© Skipping duplicate: Okkadu\n",
      "â© Skipping duplicate: Gharshana\n",
      "â© Skipping duplicate: Aadukalam\n",
      "â© Skipping duplicate: Jabardasth\n",
      "â© Skipping duplicate: Oxygen\n",
      "â© Skipping duplicate: Thikka\n",
      "â© Skipping duplicate: Gentleman\n",
      "â© Skipping duplicate: Kantri\n",
      "â© Skipping duplicate: Aaha Kalyanam\n",
      "â© Skipping duplicate: Khiladi Krishna\n",
      "â© Skipping duplicate: Yatra\n",
      "â© Skipping duplicate: Vunnadhi Okate Zindagi\n",
      "â© Skipping duplicate: Raju Gadu\n",
      "â© Skipping duplicate: Sneha Geetham\n",
      "â© Skipping duplicate: Maheshinte Prathikaaram\n",
      "â© Skipping duplicate: Sarrainodu\n",
      "â© Skipping duplicate: Gaganam\n",
      "â© Skipping duplicate: Arundhati\n",
      "â© Skipping duplicate: Eega\n",
      "â© Skipping duplicate: Aagadu\n",
      "â© Skipping duplicate: Nenu Sailaja\n",
      "â© Skipping duplicate: Lakshmi Bomb\n",
      "âœ… 34 new movie titles appended to movies.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OMDB_KEY = '2774b611'  # or os.getenv(\"OMDB_KEY\")\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# === Function to fetch basic OMDb validation ===\n",
    "def omdb_title_exists(title: str) -> bool:\n",
    "    url = f\"http://www.omdbapi.com/?t={title}&apikey={OMDB_KEY}\"\n",
    "    res = requests.get(url)\n",
    "    data = res.json()\n",
    "    return data.get(\"Response\") == \"True\"\n",
    "\n",
    "# === Agent to generate titles ===\n",
    "def generate_movie_titles(user_prompt: str, count: int) -> list:\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Generate a list of {count} real movie titles based on this request:\n",
    "\n",
    "\"{user_prompt}\"\n",
    "\n",
    "Only list the movie titles, each on a new line. Do not include extra descriptions or numbering.\n",
    "\"\"\")\n",
    "    formatted_prompt = prompt.format(user_prompt=user_prompt, count=count)\n",
    "    raw_output = llm.predict(formatted_prompt)\n",
    "\n",
    "    # Clean and split titles\n",
    "    titles = [line.strip().strip('\"') for line in raw_output.splitlines() if line.strip()]\n",
    "    return titles\n",
    "\n",
    "# === Main method ===\n",
    "def populate_movies_txt(user_prompt: str, count: int, output_file=\"movies.txt\"):\n",
    "    print(f\"ðŸŽ¬ Generating movie titles for: {user_prompt}\")\n",
    "    titles = generate_movie_titles(user_prompt, count)\n",
    "\n",
    "    # Load existing titles\n",
    "    existing_titles = set()\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_titles = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "    print(f\"ðŸ” Validating titles via OMDb...\")\n",
    "    valid_titles = []\n",
    "    for title in titles:\n",
    "        if title in existing_titles:\n",
    "            print(f\"â© Skipping duplicate: {title}\")\n",
    "            continue\n",
    "        if omdb_title_exists(title):\n",
    "            valid_titles.append(title)\n",
    "            existing_titles.add(title)  # Add to avoid future duplication\n",
    "        if len(valid_titles) >= count:\n",
    "            break\n",
    "\n",
    "    if not valid_titles:\n",
    "        print(\"âš ï¸ No new valid titles found.\")\n",
    "        return\n",
    "\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        for title in valid_titles:\n",
    "            f.write(title + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… {len(valid_titles)} new movie titles appended to {output_file}\")\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Top Hollywood movies, recent ones have higher priority, but maintain some old classical movies as well\"\n",
    "    populate_movies_txt(prompt, count=5000)  # Start with 100 during testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a33817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 0 movies to movies.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_imdb_list_titles(url, max_count=500):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    titles = []\n",
    "\n",
    "    for item in soup.select(\".lister-item-header a\"):\n",
    "        if len(titles) >= max_count:\n",
    "            break\n",
    "        titles.append(item.text.strip())\n",
    "\n",
    "    return titles\n",
    "\n",
    "def save_to_movies_txt(titles, filename=\"movies.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for title in titles:\n",
    "            f.write(title + \"\\n\")\n",
    "    print(f\"âœ… Saved {len(titles)} movies to {filename}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.imdb.com/india/top-rated-telugu-movies/\"  # Replace with your chosen IMDb list\n",
    "titles = fetch_imdb_list_titles(url)\n",
    "save_to_movies_txt(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74761666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 0.2/1.0 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.4/1.0 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 0.7/1.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.0/1.0 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.0/1.0 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting openai\n",
      "  Downloading openai-1.93.0-py3-none-any.whl (755 kB)\n",
      "     ---------------------------------------- 0.0/755.0 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 256.0/755.0 kB 7.9 MB/s eta 0:00:01\n",
      "     ----------------------- -------------- 471.0/755.0 kB 7.3 MB/s eta 0:00:01\n",
      "     -------------------------------------  747.5/755.0 kB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 755.0/755.0 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/2.1 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.6/2.1 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.9/2.1 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.1/2.1 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.5/2.1 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.8/2.1 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "     ---------------------------------------- 0.0/444.8 kB ? eta -:--:--\n",
      "     -------------------------------------  440.3/444.8 kB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 444.8/444.8 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting PyYAML>=5.3\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "     ---------------------------------------- 0.0/161.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 161.8/161.8 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66\n",
      "  Downloading langchain_core-0.3.66-py3-none-any.whl (438 kB)\n",
      "     ---------------------------------------- 0.0/438.9 kB ? eta -:--:--\n",
      "     -------------------------------- ---- 389.1/438.9 kB 23.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 438.9/438.9 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting langsmith>=0.1.17\n",
      "  Downloading langsmith-0.4.4-py3-none-any.whl (367 kB)\n",
      "     ---------------------------------------- 0.0/367.7 kB ? eta -:--:--\n",
      "     ---------------------------------- -- 337.9/367.7 kB 10.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 367.7/367.7 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "     ---------------------------------------- 0.0/73.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.5/73.5 kB ? eta 0:00:00\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.10.0-cp310-cp310-win_amd64.whl (207 kB)\n",
      "     ---------------------------------------- 0.0/207.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 207.5/207.5 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "     ---------------------------------------- 0.0/100.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 100.9/100.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\vb\\onedrive\\documents\\vscode_workspace\\cinelog_scrapers\\clip-ai-env\\lib\\site-packages (from openai) (4.14.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "     ---------------------------------------- 0.0/157.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 157.7/157.7 kB 9.2 MB/s eta 0:00:00\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "     ---------------------------------------- 0.0/129.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 129.8/129.8 kB 7.5 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\vb\\onedrive\\documents\\vscode_workspace\\cinelog_scrapers\\clip-ai-env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.8/78.8 kB ? eta 0:00:00\n",
      "Collecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting packaging<25,>=23.2\n",
      "  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.18-cp310-cp310-win_amd64.whl (134 kB)\n",
      "     ---------------------------------------- 0.0/134.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 134.6/134.6 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 54.5/54.5 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-win_amd64.whl (495 kB)\n",
      "     ---------------------------------------- 0.0/495.5 kB ? eta -:--:--\n",
      "     ----------------------------------- - 481.3/495.5 kB 15.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 495.5/495.5 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.4/2.0 MB 13.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.9/2.0 MB 11.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.3/2.0 MB 10.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.7/2.0 MB 11.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl (296 kB)\n",
      "     ---------------------------------------- 0.0/296.6 kB ? eta -:--:--\n",
      "     ------------------------------------ - 286.7/296.6 kB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 296.6/296.6 kB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\vb\\onedrive\\documents\\vscode_workspace\\cinelog_scrapers\\clip-ai-env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-inspection, tqdm, tenacity, sniffio, PyYAML, python-dotenv, pydantic-core, packaging, orjson, jsonpointer, jiter, idna, h11, greenlet, distro, charset_normalizer, certifi, async-timeout, annotated-types, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, anyio, requests-toolbelt, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 annotated-types-0.7.0 anyio-4.9.0 async-timeout-4.0.3 certifi-2025.6.15 charset_normalizer-3.4.2 distro-1.9.0 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.26 langchain-core-0.3.66 langchain-text-splitters-0.3.8 langsmith-0.4.4 openai-1.93.0 orjson-3.10.18 packaging-24.2 pydantic-2.11.7 pydantic-core-2.33.2 python-dotenv-1.1.1 requests-2.32.4 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tqdm-4.67.1 typing-inspection-0.4.1 urllib3-2.5.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e71ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "     ---------------------------------------- 0.0/187.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 187.3/187.3 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vb\\onedrive\\documents\\vscode_workspace\\cinelog_scrapers\\clip-ai-env\\lib\\site-packages (from beautifulsoup4->bs4) (4.14.0)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
